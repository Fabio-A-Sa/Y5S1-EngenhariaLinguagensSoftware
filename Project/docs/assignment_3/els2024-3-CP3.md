# ELS2024-3: CheckPoint 3

## Introduction

For this final checkpoint, our group was tasked with constructing an external DSL by using the previous Internal DSL. The language we came up with is called `PREQUEL` (preql) and it is a language focused on handling table from imported files and tables. The development of the language first started by creating its syntactic grammar using Xtext. We then exported this file in order to start creating valid .preql files.

- [Internal DSL Changes](#internal-dsl-changes)
- [External DSL Architecture](#external-dsl-architecture)
  - [Syntactic Rules](#syntactic-rules)
  - [DSL Parser](#dsl-parser)
  - [Features](#features)
- [Error handling](#error-handling)
- [Known issues and Limitations](#known-issues-and-limitations)
- [User Profile](#user-profile)
- [Conclusions](#conclusions)

## Internal DSL Changes

With the introduction of Cal's use case, it became clear to us that the internal DSL developed in the Checkpoint 2 was badly equipped to perform the desired tasks. The main changes we introduced where:

- If statements for different file extensions that defined their own behaviour;
- A new operation to obtain any number of max or min values from a file and gather them in different row columns;
- Added Global Table operations, that can average and/or sum every numerical value of every row and add them as a row at the bottom.
- Added ways to add suffixes and prefixes to column names

With these new operations and features added, Cal's use case using our Internal DSL look as such:

```java 
Table calTable = new TableBuilder()
                        .withName("Cal Table")
                        .performOperation("Op1")
                            .withImport()
                                .fromFolders("resources/assignment_3/input/")
                                    .whenExtension("yaml")
                                        .selectByTable("/total/results/dynamic", null)
                                        .end()
                                    .endWhen()
    
                                    .whenExtension("xml")
                                        .selectByTable("/total/results/static", null)
                                        .end()
                                    .endWhen()

                                    .whenExtension("json")
                                        .selectNByFilter("/functions/time%", "MAX", List.of("name #1", "time% #1", "name #1", "time% #1", "name #2", "time% #2", "name #3", "time% #3"), 3)
                                        .addColumn("Folder", "FOLDERNAME")
                                        .end()
                                    .endWhen()
                                .end()
                                .addSuffix("(Dynamic)", "iterations")
                                .addSuffix("(Dynamic)", "calls")
                                .addSuffix("(Static)", "nodes")
                                .addSuffix("(Static)", "functions")
                                .end()
                            .end()
                        .end()

                        .performOperation("Op2")
                            .sum()
                            .average()
                        .end()
                    .end()
                    .assemble("resources/assignment_3/output/output_internal_cal.html");
```

## External DSL Architecture

As stated above, the language we developed is called PREQL. It is based on SQL and as the file extension of **.preql**. The DSL's architecture is very much based on the internal DSL developed last checkpoint.
Similarly, we start by creating a Global Table and giving it a name. We then can import tables from many different files, folders or folder by specifying their path and if they are files, folder or just a folder.
After that, we can start applying an operation and can thus select tables, rows, columns and values. In a simular fashion to the internal DSL, we can apply various transformations, from sums, to filters, to adding columns and renaming them.
When all the desired operations are done, the user can finally export the table by specifying a path and the desired extension.
As we can see, our architecture is made up of 3 main parts:

- The creation of the Global Table;
- The operation phase, where operations can be used and files can be imported and changed;
- Te export phase, where the final table is merged and exported to the desired path.

Bellow we can see the use cases from Alice, Bob and Cal in our `PREQUEL` language:

### Cases

#### Alice's case

```preql
CREATE TABLE "Alice Table";

IMPORT DATA FROM FOLDER "resources/assignment_1/input/" ONLY YAML {
    SELECT COLUMN "params/criterion";
};

IMPORT DATA FROM FOLDER "resources/assignment_1/input/" ONLY YAML {
    SELECT COLUMN "params/splitter";
    ADD COLUMN "File" AS "FILENAME";
};

EXPORT TO "resources/assignment_3/output/alice.html";
```

#### Bob's case

```preql
CREATE TABLE "Bob Table";

IMPORT DATA FROM FILE "resources/assignment_2/input/vitis-report.xml" {
    SELECT TABLE "/AreaEstimates/Resources";
};

IMPORT DATA FROM FILE "resources/assignment_2/input/decision_tree.yaml" {
    SELECT TABLE * BY NON-COMPOSITE;
};

IMPORT DATA FROM FILE "resources/assignment_2/input/decision_tree.yaml" {
    SELECT TABLE "params";
};

IMPORT DATA FROM FILE "resources/assignment_2/input/profiling.json" {
    FILTER BY ("/functions/time%", MAX, ["name", "time%"], 1);
};

ADD GLOBAL COLUMN "Folder" AS "/input/" AT FIRST;

EXPORT TO "resources/assignment_3/output/bob.html";
```

#### Cal's case

```preql
CREATE TABLE "Cal Table";

IMPORT DATA FROM FOLDERS "resources/assignment_3/input/" {
    WITH EXTENSIONS {
        EXTENSION YAML {
            SELECT TABLE "/total/results/dynamic";
        };
        EXTENSION XML {
            SELECT TABLE "/total/results/static";
        };
        EXTENSION JSON {
            FILTER BY ("/functions/time%", MAX, ["name #1", "time% #1", "name #2", "time% #2", "name #3", "time% #3"], 3);
            ADD COLUMN "Folder" AS "FOLDERNAME";
        };
    };
};

ADD SUFFIX "(Dynamic)" TO "iterations";
ADD SUFFIX "(Dynamic)" TO "calls";
ADD SUFFIX "(Static)" TO "nodes";
ADD SUFFIX "(Static)" TO "functions";

ADD *;
AVERAGE *;

EXPORT TO "resources/assignment_3/output/cal.html";
```

As we can clearly see, all 3 examples are divided into the aforementioned 3 main code blocks. The only exception to these rule is in Cal's case, when he uses **ADD \*;** and **AVERAGE \*;**. These operations are used outside the scope of the OperationBuilder and are those done directly on the Global Table.

Our main goal with the external DSL's architecture was to retain the design ideas of our internal DSL. Last checkpoint, one of our goals was to make it so our Internal DSL could be quickly and easily translated and incorporated into a complete DSL and, with this checkpoint, we can say that we achieved this goal.

### Syntactic Rules

We developed our language's syntax rules through the use of **XText**. The syntax was very much inspired by SQL, but we decided to include brackets as we felt that it better represented our scope differentiation such as operations, imports and selects. This, in addition, also facilitated the parsing of the language and came in line with our internal DSL, making it, in turn, much more manageable to parse. Bellow is our Xtext file used for our language's syntax rules:

```xtext
grammar org.xtext.example.mydsl.Preql with org.eclipse.xtext.common.Terminals

generate preql "http://www.xtext.org/example/mydsl/Preql"

Model:
    statements+=Statement*;

Statement:
    CreateTable | ImportDataFile | ImportDataFolder | ImportDataFolders | DefineSuffix | DefinePrefix | OutputTable | AddColumnGlobal | RenameColumn | RemoveColumn | RowOperation;

CreateTable:
    'CREATE TABLE' name=STRING ';';

ImportDataFile:
    'IMPORT DATA FROM FILE' sourcePath=STRING '{'
        operations+=Operation*
        ('FILTER BY' filter=Filter)?
    '};';

ImportDataFolder:
    'IMPORT DATA FROM FOLDER' sourcePath=STRING ('ONLY' extType=EXT_TYPE)? '{'
        operations+=Operation*
        ('FILTER BY' filter=Filter)?
    '};';

ImportDataFolders:
    'IMPORT DATA FROM FOLDERS' sourcePath=STRING '{'
	    'WITH EXTENSIONS' '{'
	        extensions+=Extension*
	    '};'
    '};';

Extension:
    'EXTENSION' type=EXT_TYPE '{'
        ('FILTER BY' filter=Filter)?
        operations+=Operation*
    '};';

Operation:
    SelectionTable | SelectionColumn | AddColumnNested | OperationAtRows;

SelectionTable:
    'SELECT TABLE' tableName=TableName ('BY' columnFilter=COLUMN_FILTER)? ';';

SelectionColumn:
    'SELECT COLUMN' columnName=TableName ('BY' columnFilter=COLUMN_FILTER)? ';';

Filter:
    '(' targetPath=STRING ',' function=FUNCTION ',' columns+=ColumnList ',' targetN=INT ');';

AddColumnNested:
    'ADD COLUMN' columnName=STRING 'AS' columnType=STRING ('AT' columnPosition=COLUMN_POSITION)?';';

AddColumnGlobal:
	'ADD GLOBAL COLUMN' columnName=STRING 'AS' columnType=STRING ('AT' columnPosition=COLUMN_POSITION)? ';';

OperationAtRows:
	OperationType=OPERATION_TYPE column1=STRING 'AND' column2=STRING 'TO' columnFinal=STRING';';

RenameColumn:
    'RENAME COLUMN' columnName=STRING 'TO' newColumnName=STRING ';';

RemoveColumn:
    'REMOVE COLUMN' columnName=STRING ';';

RowOperation:
	rowOperation=(AddAll | AverageAll);

AddAll:
	'ADD *;';

AverageAll:
	'AVERAGE *;';

DefineSuffix:
    'ADD SUFFIX' suffix=STRING 'TO' appliesTo=STRING ';';

DefinePrefix:
    'ADD PREFIX' prefix=STRING 'TO' appliesTo=STRING ';';

OutputTable:
    'EXPORT TO' outputPath=STRING ';';

TableName:
    '*' | STRING;

ColumnList:
    '[' STRING (',' STRING)* ']';

enum COLUMN_FILTER:
	NULL | COMPOSITE = 'COMPOSITE' | NON_COMPOSITE = 'NON-COMPOSITE';

enum COLUMN_POSITION:
	NULL | FIRST | LAST;

enum OPERATION_TYPE:
	SUM | MUL | DIV | SUB;

enum EXT_TYPE:
    NULL | YAML | XML | JSON;

enum FUNCTION:
    NULL | MAX | MIN;

terminal STRING:
    '"' ('\\' . | !('\\' | '"'))* '"';
```

### DSL Parser

The `DSLParser` class is responsible for transforming and mapping instructions from the External DSL to the previously defined Internal DSL. Each token/element found in the XText tree corresponds to an instruction that will modify the internal state of the Builder and extract the necessary parameters/arguments from the node, as demonstrated in the following code:

```java
var treeIterator = resource.getAllContents();
Builder builder = new TableBuilder();

while (treeIterator.hasNext()) {

    EObject element = treeIterator.next();
            
    try {

        builder = switch (element) {
            case ModelImpl ignored -> builder;
            case CreateTableImpl createTable -> this.createTableParser(createTable, builder);
            case ImportDataFolderImpl importDataFolder -> this.importDataFolderParser(importDataFolder, builder);
            case ImportDataFoldersImpl importDataFolders -> this.importDataFoldersParser(importDataFolders, builder);
            case ImportDataFileImpl importDataFile -> this.importDataFileParser(importDataFile, builder);
            case ExtensionImpl extension -> this.extensionParser(extension, builder);
            case OutputTableImpl outputTable -> this.outputTableParser(outputTable, builder);
            case SelectionColumnImpl selectionColumn -> this.selectionColumnParser(selectionColumn, builder);
            case AddColumnGlobalImpl addColumnGlobal -> this.addGlobalColumnParser(addColumnGlobal, builder);
            case AddColumnNestedImpl addColumnNested -> this.addNestedColumnParser(addColumnNested, builder);
            case SelectionTableImpl selectionTable -> this.selectionTableParser(selectionTable, builder);
            case FilterImpl filter -> this.filterParser(filter, builder);
            case DefineSuffixImpl suffix -> this.suffixParser(suffix, builder);
            case DefinePrefixImpl prefix -> this.prefixParser(prefix, builder);
            case RowOperationImpl rowOperation -> this.rowOperationParser(rowOperation, builder);
            case RemoveColumnImpl removeColumn -> this.removeColumnOperationParser(removeColumn, builder);
            case RenameColumnImpl renameColumn -> this.renameColumnOperationParser(renameColumn, builder);
            case OperationAtRowsImpl operation -> this.operationAtRowsParser(operation, builder);
            default -> throw new DSLException("Node not implemented: " + element);
        };

    } catch (Exception exception) {
            throw new DSLException(exception.getMessage());
    }
}
```

The advantage of having modular code, with all Builders (TableBuilder, ImportBuilder, OperationBuilder, SelectorBuilder) implemented from the same generic Builder and always appearing in a chain, is that at each node we can:

- Throw a `DSLException` if something becomes inconsistent, preventing errors from being mapped to the Internal DSL and unexpected outputs from occurring;
- `Reset the builder` to the most appropriate state according to the type of node.

The `reset` is performed by each sub-parser through the following method:

```java 
private Builder reset(Builder builder, Class<?> targetClass) throws Exception {
    while (!targetClass.isInstance(builder)) {
        builder = builder.end();
    }
    return builder;
}
```

In this way, we conclude the subroutines of each Builder, implicitly implementing the closing of brackets for the external DSL. For instance:

```java 
private Builder suffixParser(DefineSuffixImpl suffix, Builder builder) throws Exception {
    builder = this.reset(builder, OperationBuilder.class);
    String suffixString = Utils.stripQuotes(suffix.getSuffix());
    String columnName = Utils.stripQuotes(suffix.getAppliesTo());
    return ((OperationBuilder) builder).addSuffix(suffixString, columnName);
}
```

### Features

In this checkpoint we added even more features compared to the last iteration. So the new added features are:

- Import files from a collection of folders;
- Differentiate the transformations applied to files depending on their extension;
- Filter by custom number of desired numbers in **MAX** and **MIN** operations;
- Add rows to the Global Table containing the average or sum of numerical values in other rows;
- Add suffixes and prefixes to the names of columns;

We added these new features to the DSL as well as made possible to use previously implemented ones in the new external DSL.

In terms of input we still support **CSV**, **JSON** and **XML**. As for output we support **HTML**, **CSV**, **JSON** and **XML** and all of them can handle recursive tables.

In terms of adding columns containing **meta** information, we still have the system of limited keywords in place. We felt that the flow we implemented would be the most commonly used, and support for more would hinder our work in more important tasks.

## Error handling

With this checkpoint we developed a sophisticated way of handling errors.

Firstly, syntactic errors are managed by the grammar developed in the **XText** file. This way, the user has direct input and keyword highlight while using our language in **.preql** files.
As for semantic errors, those are handles by the myriad of implemented exceptions in our internal DSL. When a user performs a semantic error the internal DSL will dispatch an error handler and will inform the user of their error.
With these two systems we cover both semantic and syntactic errors, as any other programming language would.

## Known issues and Limitations

Even though we implemented a number of features in this checkpoint, most of the work was done regarding an external DSL implementation. And as such many of the limitations that our project suffered in the last checkpoint still apply.

- Table-to-table operations are not supported;
- Lack of complex data operations;
- User has to be able to program in our external DSL;
- Table sort, pivoting not supported;
- Using a path to select columns and tables can be somewhat cumbersome;
- Low number of possible constraints for the SelectorBuilder, such as filtering by number ranges, other types than composite and non-composite data (strings, numbers, etc...).

The algorithms implemented last checkpoint remained largely the same. In this checkpoint we also added some additional logic to differentiate files from different extensions, this adds some cost in terms of space and time when running operations that require them.

## User Profile

Now with ou external DSL implemented, the user shifted significantly from our last checkpoint. The user now only has to grapple with the simple rules of **Preql**, which is made even easier if the user is familiar with **SQL**. The user can be compared to one who is not very familiar with programming languages but has some technical ability in manipulating data, such as someone familiar with Excel.

## Conclusions

As the Internal DSL was already developed with the External architecture in mind, its transition to `Preql` was relatively smoothly.

The DSL was designed to support all three use cases (Alice, Bob, Cal) effectively, it is feature-complete and exceeds the minimum requirements.

If future developments were to occur, they would focus on adding functionalities such as table filtering, column sorting, grouping by attributes, column aggregation, and table joins, further extending the DSL's capabilities to address more complex use cases. However, no further enhancements are currently planned.